{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OWNER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# functions for pre Processing\n",
    "def remove_comma(text):\n",
    "    return text.replace(\",\" ,\" \")  \n",
    "\n",
    "# remove url or www .com etc...\n",
    "def remove_url(text):\n",
    "    url = re.compile(r\"https?://\\S+www\\.\\S+\")\n",
    "    return url.sup(r\"\" ,text)    \n",
    "\n",
    "# remove /?}{ \\ etc....\n",
    "def remove_punc (text):\n",
    "    translator = str.maketrans(\"\",\"\" ,string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# remove an,a,the,on,that,this ,etc....\n",
    "def remove_stopwords(text):\n",
    "    filered_word = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filered_word)\n",
    "\n",
    "def counter_words(text_col):\n",
    "    count = Counter()\n",
    "    \n",
    "    for i in range(len(text_col)):\n",
    "        for word in text_col[i].split():\n",
    "            count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT TO CSV \n",
    "remve comma in commend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../JobScrapper_info/jobDiscibtion.txt\"\n",
    "\n",
    "with open(\"after.csv\" , \"w\",encoding=\"utf8\") as after:\n",
    "    with open(file_path , \"r\",encoding=\"utf8\") as befor:\n",
    "        \n",
    "        beforfile = befor.readlines()\n",
    "        for i in range(len(beforfile)):\n",
    "            removeCamma = remove_comma(beforfile[i].strip())\n",
    "            \n",
    "            after.writelines(removeCamma)\n",
    "            after.writelines(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len all commend :  546\n",
      "dict_values([19, 5, 21, 8, 16, 7, 5, 33, 11, 7, 14, 1, 2, 7, 2, 17, 3, 8, 2, 20, 17, 2, 7, 7, 1, 7, 7, 7, 5, 16, 4, 13, 14, 8, 30, 8, 31, 3, 1, 3, 2, 2, 4, 2, 13, 3, 6, 3, 13, 9, 9, 6, 8, 3, 4, 1, 1, 5, 3, 1, 1, 3, 3, 1, 2, 1, 1, 1, 1, 1, 1, 10, 4, 1, 1, 1, 1, 2, 10, 1, 1, 1, 6, 3, 4, 8, 15, 1, 22, 1, 1, 2, 3, 4, 8, 2, 2, 7, 1, 4, 1, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 13, 15, 1, 2, 8, 1, 1, 16, 1, 3, 7, 1, 6, 3, 1, 4, 1, 6, 1, 1, 3, 1, 5, 1, 2, 4, 3, 9, 6, 1, 1, 1, 1, 2, 2, 3, 2, 4, 6, 4, 5, 1, 1, 3, 1, 7, 1, 3, 2, 1, 1, 2, 2, 6, 1, 2, 4, 2, 4, 1, 1, 4, 5, 4, 8, 10, 2, 7, 10, 2, 7, 17, 1, 1, 1, 1, 3, 6, 2, 7, 4, 2, 1, 6, 2, 1, 2, 2, 1, 1, 6, 34, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 3, 2, 5, 2, 3, 3, 5, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 3, 2, 38, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 8, 7, 7, 1, 18, 4, 1, 1, 5, 3, 6, 3, 12, 2, 3, 10, 3, 3, 3, 1, 6, 2, 8, 3, 2, 3, 2, 1, 4, 4, 1, 1, 1, 1, 2, 9, 2, 2, 8, 2, 2, 1, 4, 4, 4, 4, 4, 3, 7, 2, 2, 4, 12, 1, 1, 2, 1, 1, 1, 1, 5, 1, 1, 1, 5, 11, 27, 1, 7, 7, 1, 7, 1, 1, 1, 4, 1, 7, 13, 4, 1, 1, 1, 5, 1, 5, 1, 2, 4, 3, 2, 1, 2, 6, 3, 1, 1, 2, 2, 3, 1, 1, 1, 4, 3, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 7, 3, 1, 13, 2, 9, 4, 1, 2, 1, 6, 8, 2, 4, 3, 1, 5, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 7, 1, 2, 1, 2, 1, 2, 1, 2, 1, 4, 4, 3, 1, 1, 1, 4, 2, 1, 1, 2, 1, 5, 1, 1, 1, 1, 5, 4, 1, 1, 3, 5, 2, 9, 2, 2, 2, 2, 6, 13, 4, 1, 1, 10, 6, 6, 13, 4, 7, 1, 1, 1, 5, 1, 1, 3, 2, 1, 1, 7, 1, 1, 4, 7, 1, 2, 2, 4, 2, 2, 1, 4, 3, 2, 1, 1, 5, 7, 10, 1, 4, 1, 6, 5, 2, 1, 6, 4, 1, 1, 1, 1, 2, 2, 8, 9, 1, 1, 1, 1, 1, 13, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 4, 7, 1, 3, 9, 3, 3, 1, 4, 5, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 5, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 8, 3, 2, 4, 1, 2, 3, 1, 2, 1, 2, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 5, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 3, 1, 1, 4, 1, 2, 3, 1, 1, 1, 1, 1, 4, 1, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 5, 3, 6, 3, 4, 4, 10, 2, 2, 2, 5, 2, 2, 4, 2, 2, 7, 5, 2, 1, 4, 2, 6, 2, 1, 9, 8, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 14, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 25, 12, 15, 4, 5, 2, 6, 6, 6, 3, 2, 2, 3, 2, 2, 2, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 5, 4, 3, 7, 2, 5, 2, 3, 5, 2, 3, 2, 3, 2, 8, 4, 4, 4, 4, 4, 3, 3, 3, 4, 3, 3, 2, 2, 2, 2, 2, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8])\n",
      "dict_keys(['contract', 'principal', 'engineer', 'machine', 'learning', 'verticalmove', 'inc', '·', 'united', 'states', 'remote', '6', 'hours', 'ago', '10', 'applicants', 'midsenior', 'level', '1150', 'employees', 'services', 'consulting', 'see', 'compare', '17', 'try', 'premium', 'free', 'actively', 'recruiting', 'easy', 'apply', 'save', 'share', 'show', 'options', 'job', 'member', 'magazine', '2023', 'fastestgrowing', 'private', 'companies', 'america', 'build', 'digital', 'transformation', 'product', 'software', 'engineering', 'teams', 'help', 'clients', 'achieve', 'successful', 'transformations', 'talented', 'professionals', 'reach', 'optimal', 'progression', 'throughout', 'careers', 'portfolio', 'includes', 'startups', 'financed', 'exclusive', 'venture', 'capital', 'firms', 'established', 'fortune', '500', 'salesforcecom', 'american', 'express', 'cvs', 'health', 'healthcare', 'sony', 'interactive', 'many', 'others', 'part', 'information', 'technology', 'consultancy', 'team', 'means', 'you’ll', 'access', 'industryleading', 'benefits', 'including', 'medical', 'dental', 'vision', 'life', 'insurance', 'paid', 'time', 'sick', 'vacation', 'retirement', '401k', '4', 'match', 'upon', 'dayone', 'employment', 'description', 'ai', 'play', 'crucial', 'role', 'offers', 'strategic', 'solutions', 'assesses', 'processes', 'technologies', 'provides', 'technical', 'leadership', 'organizational', 'goals', 'objectives', 'responsibilities', 'involve', 'determining', 'suitable', 'utilization', 'leading', 'implementation', 'functional', 'various', 'areas', 'organization', 'also', 'involves', 'researching', 'recommending', 'appropriate', 'application', 'intelligent', 'automation', 'address', 'business', 'needs', 'additionally', 'collaborate', 'external', 'providers', 'internal', 'responsible', 'applications', 'infrastructure', 'ensure', 'timely', 'delivery', 'highquality', 'operational', 'key', 'systems', 'pbm', 'retail', 'sectors', 'evaluating', 'cuttingedge', 'designing', 'implementing', 'innovative', 'enhance', 'efficiency', 'improve', 'client', 'satisfaction', 'ideal', 'candidate', 'possess', 'strong', 'skills', 'evaluations', 'designs', 'capable', 'driving', 'collaborative', 'efforts', 'among', 'resources', 'deliver', 'effective', 'thorough', 'understanding', 'analysis', 'simplification', 'complex', 'ability', 'define', 'implement', 'customer', 'experience', 'knack', 'conceptualizing', 'pioneering', 'articulate', 'compelling', 'results', 'invite', 'join', 'highly', 'regarded', 'additional', 'include', 'collaborating', 'project', 'management', 'office', 'accurate', 'reporting', 'escalation', 'issues', 'conducting', 'presentations', 'stakeholders', 'participating', 'vendor', 'selection', 'managing', 'relationships', 'effectively', 'analyzing', 'data', 'identify', 'patterns', 'trends', 'creating', 'test', 'harnesses', 'generating', 'synthetic', 'system', 'efficacy', 'performance', 'defects', 'required', 'qualifications', 'following', '8', 'years', 'leader', 'staff', 'architect', 'focus', 'handson', 'deep', 'least', '3', 'azure', 'cognitive', 'must', 'modeling', 'mining', 'using', 'industrystandard', 'tools', 'coding', 'python', 'preferred', 'addition', 'experiences', 'proficiency', 'scripting', 'languages', 'like', 'r', 'perl', 'well', 'objectoriented', 'java', 'net', 'developing', 'hub', 'building', 'platform', 'components', 'contributing', 'engineers', 'develop', 'training', 'ml', 'models', 'deploying', 'knowledge', 'natural', 'language', 'processing', 'computer', 'ivr', 'rpa', 'structures', 'visualize', 'tableau', 'power', 'bi', 'similar', 'packages', 'xml', 'xsd', 'found', 'post', 'company', '20169', 'followers', 'follow', '26', 'linkedin', '5000', 'list', 'period', 'two', 'decades', 'we’ve', 'global', '50', 'brands', 'smaller', 'emerging', 'growth', 'startup', 'providing', 'exceptional', 'executive', 'search', 'direct', 'hire', 'contracttemporary', 'rpo', 'built', 'complete', 'topdown', 'bottomup', 'across', 'industries', 'finance', 'healthcarelife', 'sciences', 'telecom', 'success', 'story', 'initiative', 'reinventing', 'consumer', 'banking', 'httpswwwlinkedincompulsecustomersuccessstoryrpoinitiativebuildteamleadingallen', '–', 'world’s', 'first', 'philanthropic', 'cloud', 'months', 'httpswwwlinkedincompulsecustomersuccessstoryhowwebuildworldsfirstsoftwareallen', 'innovation', 'lab', 'billiondollar', '40', 'retailer', 'within', '18', 'httpswwwlinkedincompulsecustomersuccessstoryhowwebuiltdigitalinnovationallen', '…', 'commitments', 'career', 'dedicated', 'upskilling', 'fostering', 'prioritize', 'professional', 'development', 'initiatives', 'comprehensive', 'programs', 'equip', 'new', 'ensuring', 'stay', 'ahead', 'respective', 'fields', 'provide', 'courses', 'workshops', 'certifications', 'allowing', 'expand', 'expertise', 'furthermore', 'encourage', 'continuous', 'mentoring', 'crossfunctional', 'projects', 'promoting', 'collaboration', 'skill', 'sharing', 'offer', 'personalized', 'plans', 'enabling', 'set', 'progress', 'desired', 'direction', 'recognizing', 'advancement', 'essential', 'opportunities', 'mobility', 'supporting', 'aspirations', 'strives', 'create', 'culture', 'valuable', 'members', 'learn', 'mlops', 'remotew2', 'insight', '7', '34', '134500yr', '144000yr', '10015000', 'staffing', '2', 'school', 'alumni', 'work', 'view', 'verifications', 'related', 'meet', 'hiring', 'lauren', 'schaller', '3rd', 'recruiter', 'poster', 'message', 'overview', 'large', 'oem', 'client’s', 'working', 'scalable', 'reliable', 'support', 'analytics', 'specifically', 'cutting', 'edge', 'realtime', 'streaming', 'gcp', 'utilize', 'nlp', 'techniques', 'llms', 'deploy', 'bachelor’s', 'degree', 'science', 'field', 'study', 'background', 'operations', 'specific', 'programming', 'andor', 'employerprovided', 'pay', 'range', 'exact', 'compensation', 'may', 'vary', 'based', 'location', 'base', 'salary', 'featured', '1633618', '12980', 'national', 'empowering', 'people', 'relentlessly', 'pursue', 'together', 'anything', 'possible', 'specialize', 'sourcing', 'accounting', 'delivering', 'servicebased', '1000', 'spans', '67', 'offices', 'world', 'pledged', 'place', '80000', 'jobs', 'globals', 'extend', 'far', 'beyond', 'filling', 'roles', 'diversity', 'equity', 'inclusion', 'guidance', 'specialized', 'care', 'array', 'managed', 'division', 'called', 'evergreen', 'visit', 'insightglobalcom', 'social', 'impact', 'goes', 'recognizes', 'challenges', 'consultants', 'communities', 'face', 'dei', 'employee', 'groups', 'parental', 'leave', 'mental', 'aim', 'enable', 'live', 'fullest', 'potential', 'giving', 'back', 'need', 'contributions', 'charity', 'partnerships', 'oneworld', 'taken', 'believe', 'different', 'perspectives', 'backgrounds', 'make', 'us', 'better', 'know', 'best', 'able', 'authentic', 'interactions', 'colleagues', 'journey', 'reflects', 'overall', 'workforce', 'maintaining', 'bring', 'full', 'selves', 'consultant', 'families', 'advances', 'achieving', 'real', 'recognize', 'incredible', 'value', 'brings', 'personal', 'financial', 'regardless', 'come', 'feel', 'safe', 'welcome', 'connected', 'environments', 'still', 'we’re', 'proud', 'advance', 'environmental', 'sustainability', 'carbon', 'footprint', 'relatively', 'small', 'compared', 'businesses', 'manufacture', 'products', 'however', 'firmly', 'corporate', 'responsibility', 'take', 'meaningful', 'steps', 'towards', 'getting', 'zero', 'obsessed', 'whether', 'hires', 'future', 'leaders', 'sales', 'managers', 'ongoing', 'helps', 'perspective', 'teach', 'invaluable', 'go', 'daytoday', 'grind', 'purpose', 'directly', 'tied', 'mission', 'want', 'personally', 'professionally', 'financially', 'light', 'become', 'experts', 'trade', 'state', 'art', 'spend', 'amount', 'tend', 'flame', 'worklife', 'balance', 'people’s', 'wellbeing', 'one', 'shared', 'values', '“we', 'other”', 'covid19', 'pandemic', 'solidified', 'importance', 'knew', 'could', 'fulfill', 'took', 'ceo', 'bert', 'bean', 'opened', 'dialogue', 'entire', 'inspired', 'movement', 'normalizing', 'conversations', 'ever', 'since', 'lifelong', 'pursuit', 'constantly', 'searching', 'ways', 'photos', 'previous', 'next', 'dev', 'fully', 'remotedc', 'local', 'w2', 'dice', 'reston', 'va', 'reposted', '1', 'day', '36', 'entry', 'internet', 'destination', 'tech', 'every', 'stage', 'motion', 'recruitment', 'seeking', 'via', 'today', 'nonprofit', 'industry', 'looking', 'add', 'developer', 'process', 'scanning', 'reading', 'mris', 'researchers', 'manufacturers', 'patient', 'safety', 'developments', 'image', 'longterm', 'candidates', 'dmv', 'c', 'core', 'angular', 'react', 'sql', 'bachelors', 'aws', 'breakdown', 'frontend', 'backend', 'daily', '100', 'hands', '0', 'duties', '80', 'currently', 'authorized', 'fulltime', 'basis', 'c2c', 'sponsorships', 'available', '186787', '582', 'dice’s', 'techfocused', 'marketplace', 'enables', 'connections', 'recruiters', 'technologists', 'connection', 'unrivaled', 'matching', 'capabilities', 'branding', 'speed', 'vetting', 'reduce', 'timetohire', 'establish', 'trust', 'making', 'talent', 'pipeline', 'current', 'empower', 'instant', 'messaging', 'connect', 'target', 'profiles', 'talentsearch', 'immediately', 'powerful', 'branded', 'pages', 'targeted', 'marketing', 'profile', 'detail', 'philosophy', 'strength', 'status', 'updates', 'upcoming', 'events', 'technologist', 'assist', 'featuring', 'detailed', 'titles', 'preferences', 'bestinclass', 'requirements', 'designed', 'filter', 'matches', 'transparency', 'userfriendly', 'layout', 'quickly', 'understand', 'technologist’s', 'credibility', 'sr', 'phd', '19', '65hr', '70hr', 'aptask', 'type', 'rate', 'without', 'primary', '67years', 'done', 'mathematics', 'total', 'advanced', 'statistical', 'inference', 'mathematical', 'yrs', 'implemented', 'deployed', 'model', 'deployment', 'packaging', 'lifecycle', 'byom', 'framework', 'broad', 'existing', 'algorithms', 'dl', 'frameworks', 'collection', 'quantitative', 'qualitative', 'good', 'linear', 'regression', 'logistic', 'decision', 'tree', 'random', 'forest', 'xg', 'boost', 'cat', 'ada', 'svm', 'clustering', 'etc', 'databricks', 'snowflake', 'haves', 'domain', 'would', 'plus', 'agile', 'scrum', 'environment', 'excellent', 'presentation', 'communication', 'analytical', 'problem', 'solving', 'juniortomidlevel', 'education', 'regards', 'chetna', 'dufare', 'email', 'retrieved', 'yes', 'robotics', 'perception', 'harnham', 'boston', 'days', '35', '60hr', '80hr', '201500', 'hybrid', '12', 'week', '6month', 'contracttohire', '6080hr', '130000150000', 'annual', 'position', 'developers', 'toolkit', 'easily', 'amrs', 'mobile', 'manipulators', 'rgb', 'rbgd', 'cameras', '2d', '3d', 'lidar', 'sensors', 'partnered', 'humanawareness', 'dependable', 'robots', 'logistics', 'transportation', 'newly', 'growing', 'automations', 'sensor', 'calibration', 'visual', 'slam', 'semantic', 'mapping', 'localization', 'accuracy', 'stacks', 'robot', 'communicate', 'rest', 'creative', 'ideas', 'avidlearner', 'likely', 'masters', 'minimum', 'general', 'writing', 'operating', 'ros', 'please', 'register', 'interest', 'sending', 'resume', 'link', 'page', '532203', '233', '2006', 'originally', 'emerged', 'catering', 'undergone', 'significant', 'solidifying', 'premier', '400', 'spanning', 'europe', 'kingdom', 'goto', 'provider', 'construct', 'endtoend', 'cover', 'aspect', 'extensive', 'acquisition', 'methodologies', 'partner', 'cater', 'sizes', 'unique', 'networking', 'style', 'meant', 'result', 'given', 'promoter', 'score', 'nps', '83', 'whilst', '88', 'averages', 'guides', 'seen', 'used', 'benchmark', 'drive', 'closely', 'organisations', 'in2scienceuk', 'in2scienceukorg', 'women', 'womenindataorg', 'group', 'fareeda', 'elsayed', '16', '6080', 'hourly', '130k150k', 'wellversed', 'object', 'detection', 'opportunity', 'creates', 'mind', 'robotic', 'humans', 'manner', 'multiple', 'construction', 'industrial', 'suite', 'toolset', 'customerspecific', 'theyre', 'someone', 'grow', 'ms', 'perceptionrelated', 'lidar3d', 'familiarity', 'linux', 'none'])\n",
      "len : 1047\n"
     ]
    }
   ],
   "source": [
    "with open(\"counter.txt\" , \"w\",encoding=\"utf8\") as counterFile:   \n",
    "    with open(file_path , \"r\",encoding=\"utf8\") as CommendFile:\n",
    "        all_text = CommendFile.readlines()   \n",
    "        print(\"len all commend : \",len(all_text))\n",
    "        final_text = []\n",
    "        for i in range(len(all_text)):\n",
    "            \n",
    "            preProse = remove_stopwords(remove_punc(all_text[i].strip()))\n",
    "            prepos2 = remove_comma(preProse)\n",
    "            final_text.append(prepos2)\n",
    "  \n",
    "    counter = counter_words(final_text)\n",
    "     \n",
    "     \n",
    "    print(counter.values())\n",
    "    print(counter.keys())\n",
    "    print(\"len :\",len(counter))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys_list   = list(counter.keys())\n",
    "values_list = list(counter.values())\n",
    "dfe = pd.DataFrame(zip(keys_list,values_list) ,columns=[\"keys_list\",\"values_list\"])\n",
    "dfe = dfe.sort_values(by=['values_list'], ascending=False)\n",
    "dfe.to_csv('most common word.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
